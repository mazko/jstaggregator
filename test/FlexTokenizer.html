<!DOCTYPE html>
<html>
	<head>
		<meta charset="utf-8">
		<title>QUnit FlexTokenizer tests</title>
		<link rel="stylesheet" href="qunit/qunit-1.11.0.css">
	</head>
	<body>
		<div id="qunit"></div>
		<script src="qunit/qunit-1.11.0.js"></script>
		<script src="../js/FlexTokenizer.js"></script>
		<script>
			test("empty test", function() {
				QUnit.deepEqual(new FlexTokenizer("").incrementToken(), null);
			});

			test("empty whitespase test", function() {
				QUnit.deepEqual(new FlexTokenizer(" ").incrementToken(), null);
			});

			test("simple test", function() {
				var tokenStream = new FlexTokenizer("Wha\u0301t's this thing do?"), res = [], offset = [], token;
				while (token = tokenStream.incrementToken()) {
					res.push(token.term);
					offset.push(token.offset);
				}
				QUnit.deepEqual(res, ["Wha\u0301t's", "this", "thing", "do"]);
				QUnit.deepEqual(offset, [[{start: 0, end: 7}],[{start: 8, end: 12}],[{start: 13, end: 18}],[{start: 19, end: 21}]]);
			});

			test("not 1:1 case mapping test", function() {
				var tokenStream = new FlexTokenizer(" İoo+BAR i̇ooBar\n"), res = [], offset = [], token;
				while (token = tokenStream.incrementToken()) {
					res.push(token.term);
					offset.push(token.offset);
				}
				QUnit.deepEqual(res, ["İoo", "BAR", "i̇ooBar"]);
				QUnit.deepEqual(offset, [[{start: 1, end: 4}],[{start: 5, end: 8}],[{start: 9, end: 16}]]);
			});

			test("arabic test", function() {
				var tokenStream = new FlexTokenizer("الفيلم الوثائقي الأول عن ويكيبيديا يسمى \"الحقيقة بالأرقام: قصة ويكيبيديا\" (بالإنجليزية: Truth in Numbers: The Wikipedia Story)، سيتم إطلاقه في 2008."), res = [], token;
				while (token = tokenStream.incrementToken()) {
					res.push(token.term);
				}
				QUnit.deepEqual(res, ["الفيلم", "الوثائقي", "الأول", "عن", "ويكيبيديا", "يسمى", "الحقيقة", "بالأرقام", "قصة", "ويكيبيديا",
        "بالإنجليزية", "Truth", "in", "Numbers", "The", "Wikipedia", "Story", "سيتم", "إطلاقه", "في", "2008"]);
			});

			test("chinese test", function() {
				var tokenStream = new FlexTokenizer("我是中国人。 １２３４ Ｔｅｓｔｓ "), res = [], token;
				while (token = tokenStream.incrementToken()) {
					res.push(token.term);
				}
				QUnit.deepEqual(res, ["我", "是", "中", "国", "人", "１２３４", "Ｔｅｓｔｓ"]);
			});

			test("korean test", function() {
				var tokenStream = new FlexTokenizer("안녕하세요 한글입니다"), res = [], token;
				while (token = tokenStream.incrementToken()) {
					res.push(token.term);
				}
				QUnit.deepEqual(res, ["안녕하세요", "한글입니다"]);
			});

			test("hyphen test", function() {
				var tokenStream = new FlexTokenizer("some-dashed-phrase"), res = [], token;
				while (token = tokenStream.incrementToken()) {
					res.push(token.term);
				}
				QUnit.deepEqual(res, ["some", "dashed", "phrase"]);
			});

			test("URLs test", function() {
				var tokenStream = new FlexTokenizer(
                                    "http://johno.jsmf.net/knowhow/ngrams/index.php?table=en-dickens-word-2gram&paragraphs=50&length=200&no-ads=on\n"
                                    + " some extra\nWords thrown in here. "
                                    + "http://c5-3486.bisynxu.FR/aI.YnNms/"
                                    + " samba Halta gamba "
                                    + "ftp://119.220.152.185/JgJgdZ/31aW5c/viWlfQSTs5/1c8U5T/ih5rXx/YfUJ/xBW1uHrQo6.R\n"
                                    + "M19nq.0URV4A.Me.CC/mj0kgt6hue/dRXv8YVLOw9v/CIOqb\n"
                                    + "Https://yu7v33rbt.vC6U3.XN--JXALPDLP/y%4fMSzkGFlm/wbDF4m"
                                    + " inter Locutio "
                                    + "[c2d4::]/%471j5l/j3KFN%AAAn/Fip-NisKH/\n"
                                    + "file:///aXvSZS34is/eIgM8s~U5dU4Ifd%c7"
                                    + " blah Sirrah woof "
                                    + "http://[a42:a7b6::]/qSmxSUU4z/%52qVl4\n"
                                    + "nongreedy.ru www.nongreedy.ru http://www.nongreedy.ru\n"), res = [], token;
				while (token = tokenStream.incrementToken()) {
					res.push(token.term);
				}
				QUnit.deepEqual(res, [
                                    "http://johno.jsmf.net/knowhow/ngrams/index.php?table=en-dickens-word-2gram&paragraphs=50&length=200&no-ads=on",
                                    "some", "extra", "Words", "thrown", "in", "here",
                                    "http://c5-3486.bisynxu.FR/aI.YnNms/",
                                    "samba", "Halta", "gamba",
                                    "ftp://119.220.152.185/JgJgdZ/31aW5c/viWlfQSTs5/1c8U5T/ih5rXx/YfUJ/xBW1uHrQo6.R",
                                    "M19nq.0URV4A.Me.CC/mj0kgt6hue/dRXv8YVLOw9v/CIOqb",
                                    "Https://yu7v33rbt.vC6U3.XN--JXALPDLP/y%4fMSzkGFlm/wbDF4m",
                                    "inter", "Locutio",
                                    "[c2d4::]/%471j5l/j3KFN%AAAn/Fip-NisKH/",
                                    "file:///aXvSZS34is/eIgM8s~U5dU4Ifd%c7",
                                    "blah", "Sirrah", "woof",
                                    "http://[a42:a7b6::]/qSmxSUU4z/%52qVl4",
                                    "nongreedy.ru",
                                    "www.nongreedy.ru",
                                    "http://www.nongreedy.ru"
				]);
			});

			test("emails test", function() {
				var tokenStream = new FlexTokenizer(
                                    "some extra\nWords thrown in here. "
                                    + "dJ8ngFi@avz13m.CC\n"
                                    + "kU-l6DS@[082.015.228.189]\n"
                                    + "\"%U\u0012@?\\B\"@Fl2d.md"
                                    + " samba Halta gamba "
                                    + "Bvd#@tupjv.sn\n"
                                    + "SBMm0Nm.oyk70.rMNdd8k.#ru3LI.gMMLBI.0dZRD4d.RVK2nY@au58t.B13albgy4u.mt\n"
                                    + "~+Kdz@3mousnl.SE\n"
                                    + " inter Locutio "
                                    + "C'ts`@Vh4zk.uoafcft-dr753x4odt04q.UY\n"
                                    + "}0tzWYDBuy@cSRQAABB9B.7c8xawf75-cyo.PM"
                                    + " blah Sirrah woof "
                                    + "lMahAA.j/5.RqUjS745.DtkcYdi@d2-4gb-l6.ae\n"
                                    + "lv'p@tqk.vj5s0tgl.0dlu7su3iyiaz.dqso.494.3hb76.XN--MGBAAM7A8H\n"), res = [], token;
				while (token = tokenStream.incrementToken()) {
					res.push(token.term);
				}
				QUnit.deepEqual(res, [ "some", "extra", "Words", "thrown", "in", "here",
                                                       "dJ8ngFi@avz13m.CC",
                                                       "kU-l6DS@[082.015.228.189]",
                                                       "\"%U\u0012@?\\B\"@Fl2d.md",
                                                       "samba", "Halta", "gamba",
                                                       "Bvd#@tupjv.sn",
                                                       "SBMm0Nm.oyk70.rMNdd8k.#ru3LI.gMMLBI.0dZRD4d.RVK2nY@au58t.B13albgy4u.mt",
                                                       "~+Kdz@3mousnl.SE",
                                                       "inter", "Locutio",
                                                       "C'ts`@Vh4zk.uoafcft-dr753x4odt04q.UY",
                                                       "}0tzWYDBuy@cSRQAABB9B.7c8xawf75-cyo.PM",
                                                       "blah", "Sirrah", "woof",
                                                       "lMahAA.j/5.RqUjS745.DtkcYdi@d2-4gb-l6.ae",
                                                       "lv'p@tqk.vj5s0tgl.0dlu7su3iyiaz.dqso.494.3hb76.XN--MGBAAM7A8H"
				]);
			});

			test("stress test", function() {
				var template = "1234567890ABCDEFGHIJKLMNOPQRSTUVWXYabcdefghijklmnopqrstuvwxyz", stressinput = [];
				for (var i = 0; i < 1000; i++) {
					stressinput.push(template);

					var tokenStream = new FlexTokenizer(stressinput.join("")), res = [], token;
					while (token = tokenStream.incrementToken()) {
						res.push(token.term);
					}
					QUnit.deepEqual(res, [stressinput.join("")]);

					tokenStream = new FlexTokenizer(stressinput.join(" ")); 
					res = [];
					while (token = tokenStream.incrementToken()) {
						res.push(token.term);
					}
					QUnit.deepEqual(res, stressinput);
				}
			});

		</script>
	</body>
</html>

